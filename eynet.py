# -*- coding: utf-8 -*-
"""Eynet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vqx8LVF5rTNAlpq2rgLyNZRQwY088dEA
"""

!pip install -q kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle competitions download -c diabetic-retinopathy-detection

import zipfile
import os
import pandas as pd
import os
import sys
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from skimage import io
from skimage.transform import resize
import numpy as np
import time
import numpy as np
import pandas as pd
from PIL import Image
import zipfile
import os, shutil
from skimage.transform import resize
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import MaxPooling2D
from keras.layers.convolutional import Conv2D
from keras.models import Sequential
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.model_selection import train_test_split
import keras.backend as K
from keras.preprocessing.image import ImageDataGenerator


#Data preprocessing




def create_directory(directory):
    '''
    Creates a new folder in the specified directory if the folder doesn't exist.
    INPUT
        directory: Folder to be created, called as "folder/".
    OUTPUT
        New folder in the current directory.
    '''
    if not os.path.exists(directory):
        os.makedirs(directory)




#Removing black images
def find_black_images(file_path, df):
    """
    Creates a column of images that are not black (np.mean(img) != 0)
    INPUT
        file_path: file_path to the images to be analyzed.
        df: Pandas DataFrame that includes all labeled image names.
        column: column in DataFrame query is evaluated against.
    OUTPUT
        Column indicating if the photo is pitch black or not.
    """

    lst_imgs = [l for l in df['image']]
    return [1 if np.mean(np.array(Image.open(file_path + img))) == 0 else 0 for img in lst_imgs]

#Data loader
def crop_and_resize_images(img):
    cropx = 1800
    cropy = 1800
    img_size=256
   
    y,x,channel = img.shape
    startx = x//2-(cropx//2)
    starty = y//2-(cropy//2)
    img = img[starty:starty+cropy,startx:startx+cropx]
    img = resize(img, (256,256))
    return img

cat train.zip* > train.zip

if __name__ == '__main__':
  with zipfile.ZipFile('./train.zip', 'r') as zip_ref:
    zip_ref.extractall()
  imgsList = os.listdir('./train')

  print('Train size is', len(imgsList))
  print(imgsList)

  imgsList.sort()
  imgsList.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))
  print(imgsList)

  imgsFrame = pd.DataFrame(imgsList)
  print(imgsFrame)

#crop_and_resize_images(path='./train/', new_path='./train-resized-256/', cropx=1800, cropy=1800, img_size=256)
  #crop_and_resize_images(path='./test/test/', new_path='./test-resized-256/', cropx=1800, cropy=1800, img_size=256)
  start_time = time.time()
  trainLabels = pd.read_csv('./trainLabels.csv')
  trainLabels['image'] = [i + '.jpeg' for i in trainLabels['image']]
  trainLabels['black'] = np.nan
  trainLabels['black'] = find_black_images('../data/train-resized-256/', trainLabels)
  trainLabels = trainLabels.loc[trainLabels['black'] == 0]
  trainLabels.to_csv('trainLabels_master.csv', index=False, header=True)
  print("Completed")
  print("--- %s seconds ---" % (time.time() - start_time))
  trainLabels2 = trainLabels.loc[trainLabels['black'] == 1]
  print(trainLabels2)

with zipfile.ZipFile('./trainLabels.csv.zip', 'r') as zip_ref:
    zip_ref.extractall()
  trainLabels = pd.read_csv("trainLabels.csv") 
  print(trainLabels)
  trainLabelsMergeDr = trainLabels
  for i in range(35126): 
    if trainLabelsMergeDr.loc[(i, 'level')] >= 1:
      trainLabelsMergeDr.loc[(i, 'level')] = str(1)
    else:
      trainLabelsMergeDr.loc[(i, 'level')] = str(0)

  trainLabelsMergeDr = trainLabelsMergeDr.drop("image", axis=1)   
  trainLabelsMergeDr['image'] = imgsFrame[0]

  print(trainLabelsMergeDr.head(10))
  dir = os.path.join("./train/0")
  if not os.path.exists(dir):
    os.mkdir(dir)

  dir = os.path.join("./train/1")
  if not os.path.exists(dir):
    os.mkdir(dir)

  for i in range(35126):
    if trainLabelsMergeDr.loc[(i, 'level')] == 0:
      shutil.move(os.path.join('./train/', trainLabelsMergeDr.loc[(i, 'image')]), "./train/0/")
    else:
      shutil.move(os.path.join('./train/', trainLabelsMergeDr.loc[(i, 'image')]), "./train/1/")

batch_size = 512
  nb_classes = 2
  nb_epoch = 30

  img_rows, img_cols = 256, 256
  channels = 3
  nb_filters = 32
  kernel_size = (8, 8)
  datagen = ImageDataGenerator(preprocessing_function = crop_and_resize_images, validation_split=0.1)

  train_it = datagen.flow_from_dataframe(dataframe=trainLabelsMergeDr, directory="./train", x_col='image',
                                        y_col='level', subset="training", batch_size=32,
                                        shuffle=True, class_mode='binary', target_size=(img_rows, img_cols))

  val_it = datagen.flow_from_dataframe(dataframe=trainLabelsMergeDr, directory="./train", x_col='image',
                                      y_col='level', subset="validation", batch_size=32,
                                      shuffle=True, class_mode='binary', target_size=(img_rows, img_cols))

model = Sequential()
  model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]), padding='valid', strides=1,
                    input_shape=(img_rows, img_cols, channels), activation="relu"))
  model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]), activation="relu"))
  model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Flatten())
  model.add(Dense(128))
  model.add(Activation('sigmoid'))
  model.add(Dropout(0.25))
  model.add(Dense(1))
  model.add(Activation('softmax'))

  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

stop = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=1, mode='auto')
#checkpointer = ModelCheckpoint(filepath='/content/gdrive/My Drive/Grad Project/Eyenet_Arch.h5', verbose=1, save_best_only=False)
model.fit_generator(generator=train_it, validation_data=train_it, samples_per_epoch=35126, 
                    steps_per_epoch=1000, nb_epoch=100, verbose=1, callbacks=[stop])

